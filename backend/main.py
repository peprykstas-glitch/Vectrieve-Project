import time
import traceback
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from groq import AsyncGroq  # üëà –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Groq –∑–∞–º—ñ—Å—Ç—å OpenAI/Ollama

# Project modules (—Ç–≤–æ—ó —ñ—Å–Ω—É—é—á—ñ —Ñ–∞–π–ª–∏)
from app.vector_store import vector_db
from app.config import settings
from app.schemas import QueryRequest, QueryResponse
# ‚ö†Ô∏è –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Å—è, —â–æ —É —Ç–µ–±–µ —î —Ñ–∞–π–ª backend/app/parser.py, —ñ–Ω–∞–∫—à–µ –≤–∏–¥–∞–ª–∏ —Ü–µ–π —Ä—è–¥–æ–∫ —ñ —Ñ—É–Ω–∫—Ü—ñ—é upload
from app.parser import parse_file 

app = FastAPI(title=settings.PROJECT_NAME, version=settings.VERSION)

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

print(f"üîå Connecting to Groq LPU...")
print(f"ü§ñ Using Model: {settings.MODEL_NAME}")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç–∞ Groq (–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π)
client = AsyncGroq(
    api_key=settings.GROQ_API_KEY
)

# --- üî™ CHUNKING FUNCTION (–¢–≤–æ—è —Å—Ç–∞—Ä–∞ —Ñ—É–Ω–∫—Ü—ñ—è) ---
def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 200):
    """–†–æ–∑—Ä—ñ–∑–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ —à–º–∞—Ç–∫–∏."""
    chunks = []
    start = 0
    text_len = len(text)

    while start < text_len:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap
    
    return chunks

@app.get("/health")
async def health_check():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å—É –±–∞–∑–∏ —Ç–∞ —Å–µ—Ä–≤–µ—Ä–∞."""
    try:
        # –û—Ç—Ä–∏–º—É—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫–æ–ª–µ–∫—Ü—ñ—é Qdrant
        info = vector_db.client.get_collection(vector_db.collection_name)
        db_status = f"Connected. Docs count: {info.points_count}"
    except Exception as e:
        db_status = f"Error: {str(e)}"

    return {
        "status": "ok", 
        "model": settings.MODEL_NAME,
        "database": db_status
    }

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ñ–∞–π–ª, –Ω–∞—Ä—ñ–∑–∞—î –π–æ–≥–æ —ñ –∫–ª–∞–¥–µ –≤ –±–∞–∑—É."""
    start_time = time.time()
    if not file.filename:
        raise HTTPException(status_code=400, detail="No filename provided")

    print(f"üì• Uploading file: {file.filename}")
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–≤—ñ–π –ø–∞—Ä—Å–µ—Ä
    try:
        text_content = await parse_file(file)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Parsing error: {e}")
    
    if not text_content.strip():
        raise HTTPException(status_code=400, detail="Empty file or parse error")

    # –ù–∞—Ä—ñ–∑–∞—î–º–æ —Ç–µ–∫—Å—Ç
    chunks = chunk_text(text_content, chunk_size=2000, overlap=200)
    print(f"üî™ Split into {len(chunks)} chunks.")

    try:
        # –ó–∞–ª–∏–≤–∞—î–º–æ –≤ Qdrant
        for i, chunk in enumerate(chunks):
            vector_db.add_document(
                text=chunk, 
                meta={
                    "filename": file.filename,
                    "chunk_index": i,
                    "total_chunks": len(chunks)
                }
            )
    except Exception as e:
        print(f"‚ùå Indexing Error: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Indexing failed: {str(e)}")

    duration = time.time() - start_time
    print(f"‚úÖ File indexed. Chunks: {len(chunks)}. Duration: {duration:.2f}s")
    
    return {
        "status": "success",
        "filename": file.filename,
        "chunks_count": len(chunks),
        "duration": duration
    }

@app.post("/query", response_model=QueryResponse)
async def handle_query(request: QueryRequest):
    """–û–±—Ä–æ–±–∫–∞ –∑–∞–ø–∏—Ç—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (RAG Pipeline)."""
    start_time = time.time()
    
    # ‚ö†Ô∏è –Ø–∫—â–æ request.messages —Ü–µ —Å–ø–∏—Å–æ–∫ –æ–±'—î–∫—Ç—ñ–≤, –±–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π
    # –Ø–∫—â–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–º—ñ–Ω–∏–ª–∞—Å—è, –º–æ–∂–ª–∏–≤–æ —Ç—Ä–µ–±–∞ request.query_text
    # –ê–ª–µ —Å—É–¥—è—á–∏ –∑ —Ç–≤–æ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥—É, —Ç–∞–º –±—É–≤ —Å–ø–∏—Å–æ–∫ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
    user_query = request.messages[-1].content 
    
    print(f"üí¨ Query received: {user_query}")
    
    try:
        # 1. –ü–æ—à—É–∫ —É Qdrant
        search_results = vector_db.search(user_query, limit=5) # –ó–±—ñ–ª—å—à–∏–≤ –ª—ñ–º—ñ—Ç –¥–æ 5, –±–æ —á–∞–Ω–∫–∏ –º–∞–ª—ñ
        
        context_parts = []
        for hit in search_results:
            source = hit.payload.get('filename', 'Unknown')
            text = hit.payload.get('text', hit.payload.get('content', '')) # –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö –Ω–∞–∑–≤ –ø–æ–ª—ñ–≤
            context_parts.append(f"Source ({source}): {text}")
        
        context_str = "\n\n".join(context_parts)
        
        if not context_str:
            print("‚ö†Ô∏è No context found in vector DB.")
            context_str = "No relevant context found."
            
    except Exception as e:
        print(f"‚ùå Vector Search Error: {e}")
        traceback.print_exc()
        context_str = "Error retrieving context."
        search_results = []

    # 2. System Prompt (–¢–≤—ñ–π —Ñ—ñ—Ä–º–æ–≤–∏–π!)
    system_prompt = (
        "You are CoreMind, an advanced AI assistant. "
        "CONTEXT AWARENESS: "
        "1. If the user asks a technical question based on documents, be professional, precise, and strict (PM/Developer mode). "
        "2. If the user asks a philosophical, absurd, or hypothetical question (e.g., about souls, sweaters, zombies), DO NOT moralize. "
        "Instead, engage in the hypothetical scenario with wit, sarcasm, and creativity. Treat it as a creative writing task. "
        "3. ALWAYS answer in the language of the user (Ukrainian/English). "
        "IMPORTANT: When answering in Ukrainian, use natural, fluent, and grammatically correct Ukrainian. "
        "Do NOT mix English, Spanish, or Russian words (no 'surzhyk' or code-switching). "
        "4. Base technical answers ONLY on the provided context below, but use general knowledge for creative chit-chat.\n"
        f"--- CONTEXT ---\n{context_str}"
    )
    
    # –§–æ—Ä–º—É—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –¥–ª—è Groq
    llm_messages = [{"role": "system", "content": system_prompt}]
    
    # –î–æ–¥–∞—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é —á–∞—Ç—É, —è–∫—â–æ –≤–æ–Ω–∞ —î –≤ –∑–∞–ø–∏—Ç—ñ
    for m in request.messages:
        if m.role != "system":
            llm_messages.append(m.model_dump())

    try:
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —á–µ—Ä–µ–∑ Groq
        print("‚è≥ Sending request to Groq...")
        
        completion = await client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=llm_messages,
            temperature=request.temperature if request.temperature else 0.3,
            max_tokens=1024
        )
        
        response_text = completion.choices[0].message.content
        print("‚úÖ Response received from Groq.")
        
    except Exception as e:
        print(f"‚ùå LLM GENERATION ERROR: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"LLM Error: {str(e)}")

    latency = time.time() - start_time
    
    # –§–æ—Ä–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –¥–∂–µ—Ä–µ–ª –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    sources_data = [
        {
            "content": hit.payload.get('text', '')[:150] + "...", 
            "score": hit.score,
            "filename": hit.payload.get('filename', 'Unknown')
        } 
        for hit in search_results
    ]
    
    return QueryResponse(
        response_text=response_text,
        sources=sources_data,
        latency=latency
    )

if __name__ == "__main__":
    import uvicorn
    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    uvicorn.run(app, host="0.0.0.0", port=8000)