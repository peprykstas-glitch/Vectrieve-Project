import os
from groq import AsyncGroq
import ollama
from app.core.config import settings
from app.models.schemas import QueryRequest

class LLMService:
    def __init__(self):
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Groq
        self.groq_client = None
        if settings.GROQ_API_KEY:
            try:
                self.groq_client = AsyncGroq(api_key=settings.GROQ_API_KEY)
                print(f"‚òÅÔ∏è Groq Client initialized: {settings.MODEL_NAME}")
            except Exception as e:
                print(f"‚ö†Ô∏è Groq Init Warning: {e}")

    async def generate_response(self, request: QueryRequest, context_str: str) -> tuple[str, str]:
        """
        –ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å, –≤–∏–±–∏—Ä–∞—é—á–∏ –º—ñ–∂ Cloud (Groq) —Ç–∞ Local (Ollama).
        –ü–æ–≤–µ—Ä—Ç–∞—î: (response_text, used_model_name)
        """
        # 1. –í–∏–±—ñ—Ä –ø–µ—Ä—Å–æ–Ω–∏ (Thinking Mode)
        mode_key = request.thinking_mode.lower()
        persona = settings.THINKING_MODES.get(mode_key, settings.THINKING_MODES["mentor"])
        
        # –Ø–∫—â–æ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ –∑–∞–ø–∏—Ç—ñ, –±–µ—Ä–µ–º–æ –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –ø–µ—Ä—Å–æ–Ω–∏
        temperature = request.temperature if request.temperature is not None else persona["temp"]

        # 2. –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        base_prompt = f"{persona['role']} {persona['instruction']}"
        if context_str:
            if mode_key == "auditor":
                system_prompt = (
                    f"{base_prompt} "
                    "Answer strictly using the CONTEXT below. Do not use outside knowledge. "
                    f"--- CONTEXT ---\n{context_str}"
                )
            else:
                system_prompt = (
                    f"{base_prompt} "
                    "Use the CONTEXT below as a primary source, but expand with general knowledge if needed. "
                    f"--- CONTEXT ---\n{context_str}"
                )
        else:
            system_prompt = f"{base_prompt} No specific context provided. Answer using general knowledge."

        # 3. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
        messages = [{"role": "system", "content": system_prompt}]
        for m in request.messages:
            messages.append({"role": m.role, "content": m.content})

        # 4. –õ–æ–≥—ñ–∫–∞ –≤–∏–±–æ—Ä—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (Cloud vs Local)
        force_local = (request.mode == "local") or (not self.groq_client)
        
        if force_local:
            return self._run_local(messages, temperature)
        else:
            try:
                return await self._run_cloud(messages, temperature)
            except Exception as e:
                print(f"‚ö†Ô∏è Cloud failed ({e}). Switching to LOCAL...")
                return self._run_local(messages, temperature)

    async def _run_cloud(self, messages, temperature):
        """–í–∏–∫–ª–∏–∫ Groq API"""
        print(f"‚òÅÔ∏è Using Groq ({settings.MODEL_NAME})...")
        completion = await self.groq_client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=messages,
            temperature=temperature,
            max_tokens=1024
        )
        return completion.choices[0].message.content, settings.MODEL_NAME

    def _run_local(self, messages, temperature):
        """–í–∏–∫–ª–∏–∫ Ollama (Local)"""
        print(f"üîí Using Local ({settings.LOCAL_MODEL_NAME})...")
        response = ollama.chat(
            model=settings.LOCAL_MODEL_NAME,
            messages=messages,
            options={'temperature': temperature}
        )
        return response['message']['content'], settings.LOCAL_MODEL_NAME

llm_service = LLMService()